{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBy2/Pz7seuqntbTi81eQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikolay1982Nikolaev/Inferential-Statistics-Bicocca/blob/main/Inf_5_Propertises_Random_Sample_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1. Basic Concept of Random samples:\n",
        "\n",
        "Def. 5.1.1. The r.v $X_1...X_n$ are called a random sample of size n from the population f(x) if $X_1..X_n$ are mutually independent random variables and the marginal pdf or pmf of each $X_i$ is the same function $f(x)$. ALternatively, $X_1...X_n$ are called independent and identically distributed random variables with pdf or pmf f(x). This is commopnly abbreviate to IID random variables.\n",
        "\n",
        "jpoint pdf or pmf is:\n",
        "\n",
        "$$f(x_1....x_n)= f(x_1)... f(x_n)= \\prod _{i=1}^n f(x_i)$$\n",
        "\n",
        "$$f(x_1....x_n|\\theta)= f(x_1|\\theta)... f(x_n|\\theta)= \\prod _{i=1}^n f(x_i|\\theta)$$\n",
        "\n",
        "## 5.2. Sums of Random Variables from a Random Sample\n",
        "\n",
        "Def: 5.2.1. Let $X_1..X_n$ be a random sample of size n from a population and let $T(x_1...x_n)$ be a real-valued function whose domain includes the sample space of $(X_1...X_n)$. Then the random variable or random vector $Y=T(X_1...X_n)$ is called a static. The prob distrib. of a static Y is called the sampling distribution of Y.\n",
        "\n",
        "Def: 5.2.2. The sample mean is the arithmetic average of the values in a random sample. It is usually denoted by:\n",
        "\n",
        "$$\\bar{X}= \\frac{X_1+ ...+ X_n}{n}= \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
        "\n",
        "Def: 5.2.3. The sample variance is the statistic defined by:\n",
        "\n",
        "$$S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$$\n",
        "\n",
        "The sample standard deviation is the statistic defined by $S= \\sqrt{X^2}$\n",
        "\n",
        "Theorem: 5.2.4. Let $x_1...x_n$ be any numbers and $\\bar{x}= (x_1.+ ...+ n_x)/n$. Then\n",
        "- $min_a \\sum_{i=1}^n (x_i-a)^2= \\sum_{i=1}^n (x_i - \\bar{x})^2$\n",
        "\n",
        "- $(n-1)s^2= \\sum_{i=1}^n (x_i - \\bar{x})^2= \\sum_{i=1}^2 x_i^2 = n\\bar{x}^2$\n",
        "\n",
        "Prof:\n",
        "\n"
      ],
      "metadata": {
        "id": "nuWW0z_9Tc2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemma: 5.2.5. Let $X_1...X_n$ be a random sample from a population and let $g(x)$ be a function such that $E[g(X_1)]$ and $Var[g(X_1)]$ exist. Then:\n",
        "\n",
        "$$E[\\sum_{i=1}^n g(X_i)]= n (E[g(X_1)])$$\n",
        "and\n",
        "\n",
        "$$Var[\\sum_{i=1}^n g(X_i)]= n(Var[g(X_1)])$$\n",
        "\n",
        "Prof:\n",
        "\n",
        "Theorem: 5.2.6. Let $X_1..X_n$ be a random sample from a population with mean $\\mu$ and $\\sigma^2 < \\infty$\n",
        "- $E[\\bar{X}]= \\mu$\n",
        "- $Var[\\bar{X}]= \\frac{\\sigma^2}{n}$\n",
        "- $E[S^2]= \\sigma^2$\n",
        "\n",
        "Prof:\n",
        "\n",
        "\n",
        "\n",
        "- $\\bar{X}$ is an unbiased estimator for $\\mu$\n",
        "- $S^2$ is unbiased estimator for $\\sigma^2$\n",
        "\n",
        "Theorem: 5.2.7. Let $X_1..X_n$ be a random sample from a popuylation with mgf $M_X(t)$. Then the mgf of the smaple mean is:\n",
        "$$M_{\\bar{X}}(t)= [M_X(t/n)]^n$$\n",
        "\n",
        "\n",
        "Theorem: 5.2.9. If X and Y are independent continuous random variables with pdfs $f_X(x), f_Y(y)$ then the pdf of Z= X+Y is:\n",
        "$$f_Z(z)= \\int_{-\\infty}^{\\infty} f_X(w)f_Y(z-w)dw$$\n",
        "\n",
        "Theorem: 5.2.11. SUppose $X_1..X_n$ is a random sample from a pdf or pmf $f(X|theta)$ where:\n",
        "\n",
        "$$f(x|\\theta)= h(x)c(\\theta)exp(\\sum_{i=1}^k w_i(\\theta)t_i(x))$$\n",
        " is a member of an exponential family. Define statistics $T_1...T_k$ by:\n",
        "\n",
        " $$T_i(X_1...X_n)= \\sum_{j=1}^n t_i(X_j)$$i = 1...k\n",
        "\n",
        " If the set [(w_1(\\theta)... w_k(\\theta)), \\theta \\in \\Theta] contains an open subset of $R^k$ then the distribution of $(T_1..T_k)$ is an exponential family of the form:\n",
        "\n",
        " $$f_T(u_1...u_k|\\theta)= H(u_1...u_k)[c(\\theta)]^n exp(\\sum_{i=1}^k w_i(\\theta)u_i)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "oDIBzgntTc5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3. Sampling of Normal Distribution:\n",
        "\n",
        "\n",
        "### Properties of the Sample Mean adn Variance:\n",
        "\n",
        "Theorem 5.3.1. Let $X_1...X_n$ be a random sampe from a $N(\\mu, \\sigma^2)$ distribution, and let $X=\\frac{1}{n}\\sum_{i=1}^n X_i$ and $S^2= [\\frac{1}{n-1}]\\sum_{i=1}^n (X_i-\\bar{X})^2$. Then\n",
        ". $\\bar{X}$ and $S^2$ are independet random variables\n",
        "- $\\bar{X}$ has a $N(\\mu, \\sigma^2)$ distribution\n",
        "- $(n-1)S^2 / \\sigma^2$ has a Chi-squared distribution with n-1 degrees of freedom\n",
        "\n",
        "\n",
        "Proof:\n",
        "\n",
        "Lemma 5.3.2. Facts about Chi-squared random variables: We use the notation $\\chi_p^2$ to denote a chi squared random variable with p degree of freedom.\n",
        "- if Z is a N(0,1) random variable, then $Z^2 \\sim \\chi_1^2$, that is, the square of a standard normal random variable is a chi squared variable\n",
        "- if $X_1...X_n$ are independent and $X_i \\sim \\chi_p^2$ then $X_1+...+X_n \\sim \\chi_{p_1...p_n}^2$\n",
        " that is, independet chi squared variables add to a chi squared variable, and the degrees of freedom also add\n",
        "\n",
        " Proof:\n",
        "\n",
        "\n",
        "Lemma 5.3.3. Let $X_j \\sim N(\\mu, \\sigma^2)$ j=1...n independent. FOr constants $a_{ij}$ and $b_{rj}$ j=1...n, i=1...k, r=1...m. Where $k+m\\leq n$ define\n",
        "$$U_i = \\sum_{j=1}^n a_{ij}X_j$$ i=1...k\n",
        "$$V_r= \\sum_{j=1}^n b_{rj}X_j $$ r=1...m\n",
        "\n",
        "- the r.v. $U_i, V_r$ are independent if and only if $Cov(U_i, V_r)= 0$. Furthermore $Cov(U_i, V_r)= \\sum_{j=1}^n a_{ij}b_{rj}\\sigma_j^2$\n",
        "\n",
        "- the r.vectors ($U_1...U_k$) and $(V_1..V_m)$ are independent if and only if $U_i$ is independent of $V_r$ for all pairs i,r(i=1...k; r=1...m)\n",
        "\n",
        "\n",
        "Proof:\n",
        "\n"
      ],
      "metadata": {
        "id": "9nFr22DlTc70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The derived Distributions: Student's t and Snedecor's F\n",
        "\n",
        "If $X_1...X_n$ are a random sampe from a $N(\\mu, \\sigma^2)$ we know that the quantity\n",
        "$$\\frac{X-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}$$\n",
        "is distibuited as a $N(0,1)$ r.v..If we knew the value of $\\sigma$ and we measured $\\bar{X}$, then we could use that equation as a basis ofor inference about $\\mu$, since $\\mu$ would then be the only unknown quantity.\n",
        "\n",
        "When $\\sigma$ is unknown. Student did the obvious thing- he looked at the distribution of:\n",
        "$$\\frac{X-\\mu}{\\frac{S}{\\sqrt{n}}}$$\n",
        "\n",
        "a quantity that could be used as a basis for inference about $\\mu$ when $\\sigma$ was unknown\n",
        "\n",
        "....\n",
        "\n"
      ],
      "metadata": {
        "id": "Og9GH7H1Tc-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition: 5.3.6. Let $X_1..X_n$ be a random sample a $N(\\mu, \\sigma^2)$ distibution. The quantity $\\frac{X-\\mu}{\\frac{S}{\\sqrt{n}}}$ has Student's t distribution with n-1 degrees of freedom. Equivalently, a random variable T has Student's t distribution with p degree of freedom, and we write $T\\sim t_p$ if it has pdf:\n",
        "$$f_T(t)= \\frac{...}{...}$$\n",
        "\n",
        "\n",
        "Example 5.3.5. Variance ratio distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "yWYLwn76TdAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Def 5.3.6. Let $X_1...X_n$ be a random sample from a $N(\\mu_X, \\sigma_X^2)$ population and let $Y_1...Y_m$ be a random sample from an independent $N(\\mu_Y, \\sigma_Y^2)$ population. The random variable $F= \\frac{S_X^2/ \\sigma_X^2}{\\frac{S_Y^2}{\\sigma_Y^2}}$ has Snedecor's F distribution with n-1 and m-1 degrees of freedom. Equivalently, the random variable F has the F distribution with p and q degrees of freedom if it has pdf\n",
        "\n",
        "$$f_F(x)= ---- $$\n",
        "\n",
        "Example"
      ],
      "metadata": {
        "id": "n4LiJVB-TdC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theorem: 5.3.8:\n",
        "- if $X\\sim F_{p,q}$ then $\\frac{1}{X} \\sim F_{q,p}$ that is , the reciprocal of an F random variable is again an F random variable\n",
        "- if $X\\sim t_q$ then $X^2 \\sim F_{1, q}$\n",
        "- if $X\\sim F_{p,q}$ then $(p/q)X \\ (1+(p/q)X) \\sim Beta(p/2; q/2)$"
      ],
      "metadata": {
        "id": "LUKDk-fZTdFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.4. Order Statistics:\n",
        "\n",
        "Definition: 5.4.1. The order statistics of a random sample $X_1...X_n$ are the sampek values placed in ascending order. They are denoted bt $X_{(1)}...X_{(n)}$\n",
        "\n",
        "$$X_{(1)}= min_{1\\leq i\\leq n} X_i $$\n",
        "\n",
        "$$X_{(n)}= max_{1\\leq i\\leq n} X_i $$\n",
        "\n",
        "The sample range: $R= X_{(n)}- X_{(1)}$\n",
        "\n",
        "The sample median: ....\n",
        "\n",
        "\n",
        "Definition: 5.4.2. The notation [b] , when appearing in a subscript, is defined to be the number b rounded to the nearest integer in the usual way. More precisely, if i is an integer and $i-0.5 \\leq b \\leq i+0.5$ then [b]= i\n",
        "\n",
        "Theorem: 5.4.3. Let $X_1...X_n$ be a random sample from a discrete distribution with pmf $f_X(x_i)= p_i$ where $x_1 < x_2...$ are the possble values of X in ascending order. Define\n",
        "$$P_0= 0$$\n",
        "$$P_1=p_1$$\n",
        "$$P_2= p_1 + p_2$$\n",
        "...\n",
        "$$P_i = p_1+p_2 + ..+p_n$$\n",
        "\n",
        "Let $X_{(1)}...X_{(n)}$ denote the order statistics from the sample. Then\n",
        "\n",
        "$$P(X_{(j)} \\leq x_i)= \\sum_{k=j}^n \\binom{n}{k} P_i^k (1-P_i)^{n-k}$$\n",
        "\n",
        "and\n",
        "$$P(X_{(j)} = x_i)= \\sum_{k=j}^n \\binom{n}{k}[ P_i^k (1-P_i)^{n-k} - P_{i-1}^k (1-P_{i-1})^{n-k}]$$\n",
        "\n",
        "Prof:\n",
        "\n",
        "Theorem: 5.4.4.\n",
        "\n",
        "prof\n",
        "\n",
        "\n",
        "example 5.4.5. Union order statistic pdf\n",
        "\n",
        "Theorem 5.4.6.\n",
        "\n",
        "Ex: 5.4.7. Distribution of the midrange and range\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cr-dpU4cTdHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.5. COnvergence:\n",
        "\n",
        "## 5.5.1. COnvergence in Probability\n",
        "\n",
        "Def: 5.5.1. A seq of random variables $X_1, X_2...$ converges in probability to a random variable X if, for every $\\epsilon > 0$\n",
        "\n",
        "$$lim_{n-> \\infty} P(|X_n- X|\\geq \\epsilon)= 0$$\n",
        "\n",
        "or\n",
        "\n",
        "$$lim_{n->\\infty}P(|X_n-X|< \\epsilon )=1$$\n",
        "\n",
        "Theorem: 5.5.2. Weak Law of Large Numbers\n",
        "\n",
        "Let $X_1,X_2...$ be iid random variables with $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 < \\infty$. Define $\\bar{X_n}= (1/n)\\sum_{i=1}^n X_i$. Then,\n",
        "\n",
        "for every $\\epsilon > 0$\n",
        "\n",
        "$$lim_{n-> \\infty} P(|\\bar{X}-\\mu|< \\epsilon)= 1$$\n",
        "that is, $\\bar{X_n}$ converges in probability to $\\mu$\n",
        "\n",
        "Prof:\n",
        "\n",
        "\n",
        "Example 5.5.3. Consistency of $S^2$\n",
        "\n",
        "Theorem 5.5.4. Suppose that $X_1X_2...$ converges in probability to a random variable X and that h is a continuous function. Then $h(X_1)h(X_2)$.. converges in probability oto $h(X)$.\n",
        "\n",
        "Example 5.5.5.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sHouLFW5TdJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.5.2. Almost Sure Convergence\n",
        "\n",
        "Def 5.5.6. A sequence of random varibles $X_1X_2...$ converges almost surely to a random variable X if, for every $\\epsilon > 0$\n",
        "\n",
        "$$P(lim_{n->\\infty}|X_n-X|< \\epsilon)= 1$$\n",
        "\n",
        "A r.v. is a real-valued function defined on a sample sapce S. If a sample space S has elements denoted by s, then $X_n(s)$\n",
        "and $X(s)$ are all function defined on S.\n",
        "\n",
        "...\n",
        "\n",
        "Example 5.5.7. Almost Sure convergence\n",
        "\n",
        "example. 5.5.8. Convergence in probability , not almost surely\n",
        "\n",
        "Theorem: 5.5.9. Strong Law of Large Numbers: Let $X_1X_2...$ be iid r.v with $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 < \\infty$ and define $\\bar{X}_n= \\frac{1}{n}\\sum_{i=1}^n X_i$. Then , for  every $\\epsilon > 0$:\n",
        "$$P(lim_{n->\\infty}|\\bar{X_n}-\\mu|< \\epsilon)=1$$\n",
        "\n",
        "that is $\\bar{X_n}$ converges almost surely to $\\mu$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a-kb6TZ8TdL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5.3. Convergence in Distribution:\n",
        "\n",
        "Definition: 5.5.10\n",
        "\n",
        "A sequence of random variables $X_1, X_2...$ converges in distribution to a r.v. X if:\n",
        "\n",
        "$$lim_{n-> \\infty}F_{X_n}(x)= F_X(x)$$\n",
        "at all points where $F_X(x)$ is continuous\n",
        "\n",
        "Example: 5.5.11: Maximum of uniforms\n",
        "\n",
        "Theorem: 5.5.12. If the sequence of random variables $X_1X_2..$ converges in probability to a random variable X, the sequence also converges in distribution to X.\n",
        "\n",
        "Theorem: 5.5.13. The sequence of random variables $X_1X_2...$ converges in probability to a constant $\\mu$ if and only if the sequence also converges in distribution to $\\mu$. That is, the statement:\n",
        "$$P(|X_n-\\mu|> \\epsilon)-> 0$$for every $\\epsilon > 0$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s8WKROvhTdOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Central Limit Theorem: 5.5.14\n",
        "Let $X_1,X_2...$ be a sequence of iid random variables whose mgfs exist in a neighborhood of 0 (that is, $M_{X_i}(t)$ exists for |t|< h, for some positive h). Let $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 > 0$ (Both $\\mu$ and $\\sigma^2$ are finite since the mgf exists). Define $\\bar{X_n}= \\frac{1}{n}\\sum_{i=1}^n X_i$. Let $G_n(x)$ denote the cdf of $\\sqrt{n}(\\bar{X_n}-\\mu)/\\sigma$/. Then , for any x, $-\\infty < x< ∞$\n",
        "\n",
        "$$lim_{n->\\infty}G_n(x)= \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}}exp[\\frac{-y^2}{2}]dy$$\n",
        "\n",
        "that is, $\\sqrt{n}(\\bar{X_n}-\\mu)/\\sigma$ has a limiting standard normal distribution.\n",
        "\n",
        "Prof: 5.5.14\n",
        "\n"
      ],
      "metadata": {
        "id": "aodH-GFzTdQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theorem 5.5.15: Stronger form of the Central Limit Theorem: Let $X_1X_2...$ be a sequence of iid random variables with $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 < \\infty$. Define $\\bar{X_n}=\\frac{1}{n}\\sum_{i=1}^n X_i$. Let $G_n(x)$ denote the cdf of $\\sqrt{n}(\\bar{X_n}-\\mu)/\\sigma$.\n",
        "\n",
        "Then , for any x: $-\\infty < x < \\infty$\n",
        "\n",
        "$$lim_{n->\\infty}G_n(x)= \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}exp[-\\frac{y^2}{2}]dy$$\n",
        "\n",
        "that is: $\\sqrt{x}(\\bar{X_n}-\\mu)/\\sigma$ has a limiting standard normal distribution\n",
        "\n",
        "Eample: Normal Approximation to the negative binomial\n",
        "\n"
      ],
      "metadata": {
        "id": "GNMxDSsHTdSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SLutsky Theorem: 5.5.17: if $X_n -> X $ in distribution and $Y_n-> a$ a constant , in probability , then:\n",
        "\n",
        "- $Y_nX_n -> aX$ in distribution\n",
        "- $X_n + Y_n -> X+a$ in distribution\n",
        "\n",
        "Example 5.5.18: Normal Approximation with estimated variance\n",
        "\n"
      ],
      "metadata": {
        "id": "hUlGdieITdVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delta method: 5.5.4.\n",
        "\n",
        "Example 5.5.19. Estimating the odds\n",
        "\n",
        "Definition: 5.5.20: If a function g(x) has derivatives of order r, that is $g^{(r)}(x)= \\frac{d^r}{dx^r}g(x)$ exists, then for any constant a, the Taylor polynomial or order r about a is ⁉\n",
        "\n",
        "$$T_r(x)= \\sum_{i=1}^r \\frac{g^{(i)}(a)}{i!}(x-a)^i$$\n",
        "\n",
        "Theorem 5.5.21 Taylkor\n",
        "\n",
        "Theorem: 5.5.24. Delta Method: Let $Y_n$ be a sequence of random variables that setisfies: $\\sqrt{n}(Y_n-> \\theta) -> N(0, \\sigma^2)$ in distribution. For a given function g and a specific value of $\\theta$ , suppose that $g'(\\theta)$ exists and is not 0. Then\n",
        "\n",
        "$$\\sqrt{n}(g(Y_n)- g(\\theta))-> N(0, \\sigma^2[g'(\\theta)]^2)$$ in distribution\n",
        "\n",
        "Prof:\n",
        "\n",
        "Theorem: second order Delta method\n",
        "\n",
        "Theorem: multivariate Delta method\n",
        "\n"
      ],
      "metadata": {
        "id": "AZUza3ROTdXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NzSM62r2TdZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.6. Generating a Random Sample:\n",
        "\n",
        "Ex: 5.6.1. Exponential Lifetime:\n",
        "\n",
        "5.6.1. DIrect Method:\n",
        "\n"
      ],
      "metadata": {
        "id": "rv8Nc7YxTdb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.6.2. Indirect Method: Accept Rejection\n",
        "\n",
        "Theorem: 5.6.8. Let $Y\\sim f_Y(y)$ and $V \\sim f_V(V)$ where $f_Y$ and $f_V$ have common support with:\n",
        "\n",
        "$$M = sup_y f_Y(y)/f_V(y) < \\infty$$\n",
        "\n",
        "To generate a random variable $Y \\sim f_Y$:\n",
        "- Generate $U \\sim uniform (0,1), V\\sim f_V, independent$\n",
        "- if $U < \\frac{1}{M}f_Y(V)\\f_V(V)$ set Y=V, otherwise , return to step a\n",
        "\n"
      ],
      "metadata": {
        "id": "LykTSscuTdeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jxTmtw1vTdgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aubvnqWZTdix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zbitxzvJTdku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "onMLitVeTdnF"
      }
    }
  ]
}