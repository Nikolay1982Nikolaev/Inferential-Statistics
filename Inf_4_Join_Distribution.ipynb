{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7hzruoqIaej97VKxUie0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikolay1982Nikolaev/Inferential-Statistics-Bicocca/blob/main/Inf_4_Join_Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Join and Marginal Distribution:\n",
        "\n",
        "Multivariate models:\n",
        "\n",
        "Def 1.4.1 univariate random variable\n",
        "\n",
        "DEf.4.1.1 An n- dimensional random vector is a function from a sample space S into $R^n$ n-dimensional Euclidian space\n",
        "\n",
        "(X, Y)\n",
        "\n",
        "discrete random vector: f(x, y) = P(X=x, Y=y)\n",
        "\n",
        "Def: 4.1.3. Let (X, Y) be a discrete bivariate random vector. Then the fuction f(x, y) from $R^n$ into R defined by $f(x, y)== P(X=x, Y=y)$ is called the **joint probability mass function** or joint pmf of (X, Y). If it is necessary to stress the fact that f is the joint pmf of the vector (X, Y) , rather than some other vector, the notation $F_{X, Y}(x, y)$ will be used\n",
        "\n",
        "$$P((X, Y) \\in A)= \\sum_{(x, y)\\in A} f(x, y)$$\n",
        "\n",
        "$$P(X= 7, Y \\leq 4)= P((X< Y) \\in A)= f(7,1) + f(7, 3)= 1/18 + 1/18 = 1/9$$"
      ],
      "metadata": {
        "id": "UQH8Fy5AlCup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theorem: 4.1.6. Let (X, Y) be a discrete bivariate random vector with joint pmf $f_{X, Y}(x, y)$. Then the marginal pmf-s of X and Y , $f_X(x)=P(X=x)$ and $f_Y(y)= p(Y=y)$ are given by:\n",
        "$$f_X(x)= \\sum_{y \\in R} f_{X, Y}(x, y)$$\n",
        "$$f_Y(y)= \\sum_{x \\in R} f_{X, Y}(x, y)$$\n",
        "\n",
        "Prof:\n",
        "\n"
      ],
      "metadata": {
        "id": "dXIEUYLvlCxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition: 4.1.10. A function f(x, y) from $R^n$ into R is called a joint probability density function or joint pdf of the continuous bivariate random vector (X, Y) , if for every $A \\subset R^2$:\n",
        "\n",
        "$$P((X, Y) \\in A) = \\int_A \\int(x, y)dx dy$$\n",
        "\n"
      ],
      "metadata": {
        "id": "LVeix7XGlCzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marginal probability density function of X and Y are also defined as in the discrete case with integrals replacing sums:\n",
        "\n",
        "$$f_X (x)= \\int_{-\\infty}^{\\infty} f(x, y) dy; -\\infty < x< \\infty$$\n",
        "$$f_Y (y)= \\int_{-\\infty}^{\\infty} f(x, y) dx; -\\infty < x< \\infty$$\n",
        "\n",
        "Any function f(x, y) satisfying $f(x, y \\geq 0$ for all $(x, y) \\in R^2$) and\n",
        "\n",
        "$$1= \\int_{-\\infty}^\\infty int_{-\\infty}^\\infty f(x, y) dx dy$$ is the joint pdf of some continuous bivariate random vecor (X, Y)\n",
        "\n"
      ],
      "metadata": {
        "id": "uVh1viNdlC2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2. COnditional Distibution and Independence\n",
        "\n",
        "X- height\n",
        "\n",
        "Y - weight\n",
        "\n",
        "\n",
        "Def: 4.2.1. Let (X, Y) be a discrete bivariate random vector with joint pmf f(x, y) and marginal $f_X(x)$ and $f_Y(x)$. For any x such that $P(X= x)= f_X(x) > 0$ the conditional pmf of Y given that X=x is the function of y denoted by $f(y|x)$ and defined by:\n",
        "\n",
        "$$f(y|x)= P(Y=y|X=x)= \\frac{f(x, y)}{f_X(x)}$$\n",
        "\n",
        "For any y such that $P(Y=y) = f_Y = f_Y(y) > 0 $ the conditional pmf of X given that Y = y is the function of x by f(x|y) and defined by:\n",
        "\n",
        "$$f(x|y)= P(X=x|Y=y)= \\frac{f(x, y)}{f_Y(y)}$$\n",
        "\n",
        "Since we have called $f(y|x)$ a pmf , we should  verify that this function of y does indeed define a pmf for a random variable. First $f(y|x) \\geq 0$ fir every y since $f(x, y) \\geq 0$ and $f_X(x)> 0$ . Second:\n",
        "\n",
        "$$\\sum_{y} f(y|x)= \\frac{\\sum_y f(x, y) }{f_X(x) }= \\frac{f_X(x)}{f_X(x)}= 1$$\n",
        "\n",
        "Thus $f(y|x)$ is indeed a pmf and can be used in the usual way to compute probabilities involving Y given the knowladge that X=x occurred.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fgpmYfxdlC40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Def 4.2.3. Let (X, Y) be a continuous bivariate random vector with joint pdf f(x, y) and marginal pdfs $f_X(x)$ and $f_Y(y)$. For any x such that $f_X(x) > 0 $, the conditional pdf of Y given that X=x is the function of y denoted by f(y|x) and defined by:\n",
        "\n",
        "$$f(y|x)= \\frac{f(x, y)}{f_X(x)}$$\n",
        "\n",
        "For any y...."
      ],
      "metadata": {
        "id": "SRPK5IGNlC74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional expected value of g(Y) given that X= x is denoted by $$E[g(Y)|x] = \\sum_y g(y)f(y|x)$$ and $$E[g(Y)|x]= \\int_{-\\infty}^{\\infty} g(y)f(y|x)dy$$\n",
        "\n",
        "$$E[Y|X= x]= \\int_x^{\\infty} ye^{-(y-x)} dy = 1+ x $$\n",
        "\n",
        "$$Var[Y|x] = E[Y^2|x] - (E[Y|x])^2 $$\n",
        "\n",
        "\n",
        "Deintiont 4.2.5. Let (X< Y) be a bivariate random vector with joit pdf or pmf f(x, y) and marginal pdfs or pmfs $f_X(x), f_Y(y)$. Then X and Y are called indepndent random variables if, for every $x\\in R$ and $y \\in R$\n",
        "\n",
        "$$f(x, y)= f_X(x) f_y(y)$$\n",
        "\n",
        "If X and Y are indepdendent , the conditional pdf of Y given X=x is: $$f(y|x)= \\frac{f(x, y)}{f_X(x)}= \\frac{f_X(x)f_Y{y}}{f_X(x)}= f_Y(y)$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nVslFXAmlC-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemma 4.2.7. Let (X, y) be a bivariate random vector with joint pdf or pmf f(x, y). Then X and y are independent random variables if and only if there exist functions g(x) and h(y) such that, for every $x\\in R$ and $u \\in R$\n",
        "$$f(x, y)= g(x)h(y)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "jX2IkkuMlDAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theorem: Let X and Y be independent random variables:\n",
        "\n",
        "a. For any $A \\subset R$ and $B \\subset R, P(X\\ in A, Y \\in B)= P(X\\in A)P(Y \\in B)$  that is the event $[X \\in A]$ and $[X \\in B]$ are independent events:\n",
        "\n",
        "b. Let g(x) be a function only of x and h(y) be a function only of y/ Then\n",
        "\n",
        " $$E[g(X)h(Y)]= (E[g(X)])(E[g(Y)])$$"
      ],
      "metadata": {
        "id": "8NgkRmKMlDDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thgeorem: 4.2.12. Let X and Y be independent random variables with moment generating functions $M_X(t)$ and $M_Y(t)$. Then the moment generationg function of the random variable Z= X+Y is given by:\n",
        "\n",
        "$$M_Z (t)= M_X(t)M_Y(t) $$\n",
        "\n"
      ],
      "metadata": {
        "id": "71I1dV5llDGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theorem: 4.2.14 Let $X\\sim N(\\mu, \\sigma^2)$ and $Y\\sim N(\\gamma; \\tau^2)$ be independent normal variables. Then the random variables Z= X+Y has a $N(\\mu +\\gamma; \\sigma^2 + \\tau^2)$ distribution\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AKSFO762lDIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Bivariate Transformations:\n",
        "\n",
        "Theorem: 4.3.2. If $X \\sim Poisson(\\theta)$ and $Y \\sim Poisson(\\lambda)$ and X and Y are indpendent , then $X+Y \\sim Poisson(\\theta + \\lambda)$\n",
        "\n",
        "Thewoem: 4.3.5. Let X and Y be independet random variables. Let g(x) be a function only of x and h(y) be a function only of y. Then the random variables U=g(X) and V=h(Y) are independent\n",
        "\n"
      ],
      "metadata": {
        "id": "Soif_jlHlDLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Hierarchical Models and Mixture Distributions:\n",
        "\n"
      ],
      "metadata": {
        "id": "fyNZ7m6OlDOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ingeneral, a r.v can have only one distribution, it is often easier to model a situatuin\n",
        "\n",
        "- large number of eggs - Poisson(\\lambda)\n",
        "- survive eggs- bernoulli\n",
        "\n",
        "$$X(num.survives)|Y \\sim binomial (Y, p)$$\n",
        "$$Y \\sim Poisson(\\lambda)$$\n",
        "\n",
        "\n",
        "Theorem: 4.4.3. If X and Y are any two r.v. then:\n",
        "$$E[X]= E[E[X|Y]]$$ provided that the expectation exist\n",
        "\n",
        "Mixture distribution - refers to a distribution arising from a hierarchical structur.\n",
        "\n",
        "Def: 4.4.4. A r.v X is said to have a mixture distribution f the distribution of X depends on a quantity also has a distribution\n",
        "\n",
        "Theorem 4.4.7 COnditional varinace identity: for any two random variables X and Y:\n",
        "$$Var[X]= E[Var[X|Y]]+ Var[E[X|Y]]$$\n",
        "provided that the expectations exist.\n",
        "\n"
      ],
      "metadata": {
        "id": "8z_WDVYzlDQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covariance and Correlation:\n",
        "\n",
        "Def: 4.5.1. The covariance of X and Y is the number defined by:\n",
        "$$Cov(X, Y)= E[(X-\\mu_X)(Y-\\mu_Y)]$$\n",
        "\n",
        "Def.4.5.2. The correlation of X and Y is the numbner defined by\n",
        "$$\\rho_{XY}= \\frac{Cov(X, Y)}{\\sigma_X; \\sigma_Y}$$\n",
        "- $\\rho_{X< Y}$ is correlation coefficient\n",
        "\n",
        "Theorem: 4.5.3. For any random variables X and Y:\n",
        "$$Cov(X< Y)= E[XY]-\\mu_X \\mu_Y$$\n",
        "\n",
        "Prof:\n",
        "\n",
        "\n",
        "Theorem: 4.5.5. If X and Y are independent random variable , then Cov(X, Y)=0 and $\\rho_{XY}=0$\n",
        "\n",
        "Prof:\n",
        "\n",
        "To understand the sums of r.v.\n",
        "\n",
        "Theorem: 4.5.6: if X and Y are any two r.v. and a and b are any two constant, then:\n",
        "\n",
        "$$Var[aX+ bY]= a^2Var[X]+ b^2Var[Y] + 2abCov[X, Y]$$\n",
        "\n",
        "If X and Y are independent variables then:\n",
        "$$Var(aX +bY)= a^2Var[X]+ b^2Var[Y]$$\n",
        "\n",
        "Prof:\n",
        "\n",
        "The nature of the linear relationship measured by cov and corr is somewhat explained by:\n",
        "\n",
        "somewhat - do izvestna stepen\n",
        "\n",
        "Theorem: 4.5.7. For any random variables X and Y\n",
        "- $-1 \\leq \\rho_{XY} \\leq 1$\n",
        "- |\\rho_{XY}=1| if and only if there exist numbers $a \\ne 0$ and b such that $P(Y=aX + b)=1$. If $\\rho_{XY}=1$ then a>0 and if $\\rho_{XY} \\leq 1$, then a < 0\n",
        "\n",
        "Prof:\n",
        "\n",
        "\n",
        "\n",
        "## Covariance inequality: 4.7.9\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GEPZ-dUSlDTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. Multivariate Distributions:\n",
        "\n",
        "The random vector $X= (X_1..X_n)$ has a sample space that is a subset of $R^n$\n",
        "\n",
        "If $(X_1...X_n)$ is a discrete random vector ==the sample space is contable, the the **joint pmf** of $(X_1...X_n)$ is the function def by $f(x)= f(x_1...x_n)= P(X_1=x1...X_n=x_n)$ for each $(x_1...x_n) \\in R^n$. The for any $A \\subset R^n$\n",
        "$$P(X \\in A)= \\sum_{x\\in A} f(x)$$\n",
        "\n",
        "if continuos:\n",
        "\n",
        "$$P(X \\in A)= \\int ...\\int_A f(x)dx= \\int ...\\int_A f(x_1...x_n)dx_1...dx_n$$\n",
        "\n",
        "\n",
        "Def: 4.6.2. Let a and m be positive integers and let $p_1..p_n$ be numbers satisfying $0 \\leq p_i \\leq 1$, i=1...n and $\\sum_{i=1}^n p_i=1$. Then the random vector ($X_1...X_n$) has a **multionial distribution with m trials and cell probabilities** $p_1...p_n$ if the joint pmf of $(X_1...X_n)$ is\n",
        "$$f(x_1...x_n)= \\frac{m!}{x_1!...x_n!}p_1^{x_1}...p_n^{x_n}= m! \\prod_{i=1}^n \\frac{p_i^{x_i}}{x_i!}$$\n",
        "\n",
        "on the set of $(x_1...x_n)$ such thjat each $x_i$ is a nonnegative integer and $\\sum_{i=1}^n x_i = m$\n",
        "\n",
        "Theorem: 4.6.4. Multinomial Theorem: let m and n be a positive integers. Let A be the set of vectors $x= (x_1...x_n)$ such each $x_i$ is a nonnegative integer and $\\sum_{i=1}^n x_i= m$. Then , for any real numbers $p_1...p_n$:\n",
        "\n",
        "$$(p_1+ ...+ p_n)^m = \\sum_{x\\in A}\\frac{M!}{x_1! ...x_n!}p_1^{x_1}....p_n^{x_n}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "MOd6HXTclDWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition: 4.6.5. Let $X_1..X_n$ be random vectors with joint pdf and pmf $f(x_1..x_n)$. Let f_X(x_i) denote the marginal pdf or pmf of $X_i$. Then $X_1..X_n$ are called mutually independent random vectors if, for every $(x_1...x_n)$\n",
        "$$f(x_1..x_n)= f_{X_1}(x_1)...f_{x_n}(x_n)= \\prod_{i=1}^n f_{X_i}(x_i)$$\n",
        "\n",
        "If the $X_i$ are all one-dimension, then $X_1..X_n$ are called mutually independent random variables.\n",
        "\n",
        "Theorem: 4.6.6. Generalization of Theorem 4.2.10. Let $X_1...X_n$ be mutually independent random variables. Let $g_1...g_n$ be real -valued functions such that $g_i(x_i)$ is a function only of $x_i$ i=1...n. Then:\n",
        "$$E[g_1(X_1)....g_n(X_n)]= (E[g_1(X_1)])...(E[g_n(X_n)])$$\n",
        "\n",
        "\n",
        "Theorem generalization of 4.2.12: Let $X_1...X_n$ be mutually independent random variables with mgfs $M_{X_1}(t)....M_{X_n}(t)$. Let $Z = X_1+ ...X_n$. The the mgf if Z is:\n",
        "\n",
        "$$M_Z(t)=  M_{X_1}(t)....M_{X_n}(t)$$\n",
        "\n",
        "In particular, if $X_1...X_n$ all have the same distribution with mgf $M_X(t)$, then $$M_Z(t)= (M_X(t))^n$$\n",
        "\n",
        "\n",
        "Theorem: generalization of Lemma 4.2.7.: Let $X_1...X_n$ be random vectors. Then $X_1...X_n$ are mutually independent random vectors if and only if there exist functions $g_i(x_i)$, i=1...n, such that the joint pdf or pmf of $(X_1...X_n)$  can be written as:\n",
        "\n",
        "$$f(x_1...x_n)= g_1(x_1)...g_n(x_n)$$\n",
        "\n",
        "\n",
        "Theorem 4.6.12. Generalization of theorem 4.3.5: Let $X_1..X_n$ be independent random vectors. Let $g_i(x_i)$ be afunction only of $x_i$, i=1..n. Then the ranodm variables $U_i= g_i(X_i)$ are mutually independent.\n",
        "\n"
      ],
      "metadata": {
        "id": "uVEj6mN-lDY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XPTyJpOVlDbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lnwnZx7AlDdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rQ9NJNCXlDgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FJNb_-emlDis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c6j43UULlDlh"
      }
    }
  ]
}